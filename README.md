# ai_safety_checker.py
  "A collection of Python scripts designed to evaluate LLM outputs for security vulnerabilities and ensure AI-generated code follows safety best practices (Red Teaming approach)."
